llm:
  provider: "groq"  # groq, openai, anthropic, etc.
  model: "meta-llama/llama-4-scout-17b-16e-instruct"  # Default Groq model
  temperature: 0.7
  max_tokens: 2000

mcp_server:
  host: "localhost"
  port: 8000

streamlit:
  title: "Human Digital Twin"
