llm:
  provider: "groq"  # groq, openai, anthropic, etc.
  model: "meta-llama/llama-4-scout-17b-16e-instruct"  # Default Groq model
  temperature: 0.7
  max_tokens: 2000

mcp_server:
  host: "localhost"
  port: 8000

streamlit:
  title: "Human Digital Twin"

ontology:
  schema_url: "https://schema.org/version/latest/schemaorg-current-https.jsonld"
  schema_path: "data/ontology/schema.jsonld"
  cache_dir: "data/ontology/cache"
  embedding_provider: "cohere"  # cohere, mistral, gemma, qwen
  validation_threshold: 0.5  # Minimum confidence score for valid triplets
  rate_limit_delay: 2.0  # Delay between embedding API calls (seconds) - 2.0+ recommended for Cohere trial (prevents 429 errors)

sessions:
  sessions_dir: "data/sessions"  # Directory for saving triplet extraction sessions
  auto_save: true  # Automatically save triplets after extraction

knowledge_graph:
  storage_type: "neo4j"  # "neo4j" or "in_memory"
  neo4j:
    uri: "bolt://localhost:7687"
    database: "neo4j"  # Default database name
    # Username and password are loaded from .env: NEO4J_USERNAME, NEO4J_PASSWORD
